# Large Scale Dynamic Machine Learning WorkFlow
## Abstract
Big data has become a large part of machine learning nowadays. As the size of data is increasing exponentially, there is a need to scale this data using a suitable machine learning algorithm. In the real world, it's not easy to scale this vast data using traditional stand-alone algorithms. And this is how a distributed algorithm has come to existence and improved execution time and system performance. Our research-based idea is to work on these distributed algorithms across three different architectural frameworks: Apache Spark, Distributed TensorFlow (DTF), and Message Passing Interface (MPI). In Spark set up, all the data and its parameters (x, y) are stored on a Resilient Distributed Dataset (RDD) and computed. DTF follows a parameter server approach, where master initializes the session to associate the tasks to workers. MPI makes use of collective communication constructs to establish communication between the master and worker processes. Our goal is to obtain a better result in a faster time by evaluating the performance on four different test problems, using this architecture in distributed settings.
We have trained our distributed algorithm on 1) Weblogs, 2) Recommender systems, 3) Topic Modeling, and 4) Deep Residual Learning problems. From the results, the spark took less execution time. It produced better results on the first three test case study problems due to its tunable knob faster memory management and execution strategies. Although DTF and MPI have shown similar results as the spark, more execution time took to produce those results as these frameworks donâ€™t perform in-memory cluster computing like a spark. Finally, in our last case study, we presented a custom deep residual learning framework by linking to ResNet-18 architecture and trained on DTF across multiple workers.
